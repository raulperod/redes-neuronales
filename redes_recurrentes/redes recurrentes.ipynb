{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Redes recurrentes, implementación simple\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 9 de mayo de 2018.\n",
    "\n",
    "En esta libreta vamos a explorar como desarrollar redes recurrentes, simples y basadas en unidades de memoria de largo y corto plazo (LSTM), aplicadas a la generación automática de texto.\n",
    "\n",
    "Dado que estamos en México en año electoral, y que se vota por diputados y presidentes municipales en muchisimos municipios del país, nos preguntamos si podríamos inventar nuevos municipios para que todos los candidatos tuvieran un lugar que gobernar. Así, generamos una lista con el nombre de todos los municipios de México, y la vamos a usar para aprender los nombres, y generar nombres a partir de una red recurrente. Esto es interesante ya que en México hay muchos municipios cuyos nombres tienen raices del español, el nahuatl, direrentes lenguas mayas, varias lenguas de la familia yuto-azteca, e inclusive algunos que son palabras inventadas (como Mexicali). Así que generar nombres de municipios mexicanos *creibles* es un problema interesante.\n",
    "\n",
    "El archivo con el nombre de los municipios se encuentra para su descarga [aqui](../varios/municipios.txt).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes recurrentes: Desarrollar una red recurrente completamente a pie.\n",
    "\n",
    "Con el fin de entender como funcionan las redes neuronales, vamos a aplicar un modelo de generacion de texto *letra a letra*, en el cual tanto la arquitectura como el método de aprendizaje sean programados a mano. \n",
    "\n",
    "No vamos a pedir que lo programen todo solos, simplemente que utilicen el modelo programados en este [gist](https://gist.github.com/karpathy/d4dee566867f8291f086), y lo adapten a leer el nombre de los municipios y a generar nombres de municipios.\n",
    "\n",
    "Para esto:\n",
    "\n",
    "1. Copiiar el contenido del *gist* y comentarlo en español (y cambiar algo de código de forma que quede mas claro para ti y para mi).\n",
    "\n",
    "2. Copiar y comentar en español el contenido del método de verificción de gradiente (para limpiar el código de errores) y usarlo por unas cuantas iteraciones para demostrar que el algoritmo de entrenamiento funciona correctamente.\n",
    "\n",
    "3. Ajustar los hiperparámetros del modelo, así como los parámetros del algoritmo de entrenamiento con el fin de generar una lista de nombres de municipios creibles, pero sin sobreaprendizaje (esto es, que copie vilmente el nombre de municipios existentes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar aqui el código, y en cuantas celdas como consideres necesarias\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):    \n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 35530 characters, 68 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('municipios.txt', 'r', encoding=\"utf8\").read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 80000, loss: 35.915278\n",
      "iter 120000, loss: 35.000723\n",
      "iter 160000, loss: 32.206458\n",
      "----\n",
      " María Tempamoxmitula\n",
      "Santa María Tancatin\n",
      "Jocote\n",
      "Santo El Salte Hoxcotlán\n",
      "Santa María Yolopa\n",
      "Santa María Tetzal\n",
      "Santaran Oiulán\n",
      "Santa Mería Yampana\n",
      "Sanan Mixtepec\n",
      "Santa María Yácuda\n",
      "Santa María Tñalit \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "max_iter = 200000\n",
    "while n < max_iter:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "    #print('inputs: ', inputs)\n",
    "    #print('targets: ', targets)\n",
    "    \n",
    "    # sample from the model now and then\n",
    "    if n == max_iter - 1:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % (max_iter/5) == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes recurrentes tipo LSTM\n",
    "    \n",
    "Las redes con unidades LSTM las vimos platicadas en clase, pero no hay nada mejor para entender un tema que implementarlo y comparar los resultados con la red recurrente simple, sin memoria, para esto vamos a hacer lo mismo que antes, pero con unidades LSTM.\n",
    "\n",
    "Para esto vamos a utilizar otro [gist](https://gist.github.com/karpathy/587454dc0146a6ae21fc) del mismo autor (que es una referencia obligada en el tema, por cierto). En este *gist*, el autor presenta un modelo de redes recurrentes LSTM desarrollado con *numpy* incluido el método de entrenamiento, pero no lo aplica a el modelado *letra a letra* como el gist pasado. Para esta parte de la libreta, lo que tienen que realizar es lo siguiente:\n",
    "\n",
    "\n",
    "1. Copiiar el contenido del *gist* y comentarlo en español (y cambiar algo de código de forma que quede mñas claro para ti y para mi).\n",
    "\n",
    "2. Adaptar el modelo propuesto para usarlo en la generación de nombres de municipios.\n",
    "\n",
    "3. Ajustar los hiperparámetros del modelo, así como los parámetros del algoritmo de entrenamiento con el fin de generar una lista de nombres de municipios creibles, pero sin sobreaprendizaje (esto es, que copie vilmente el nombre de municipios existentes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar aqui el código, y en cuantas celdas como consideres necesarias\n",
    "\"\"\"\n",
    "This is a batched LSTM forward and backward pass\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "  \n",
    "    @staticmethod\n",
    "    def init(input_size, hidden_size, fancy_forget_bias_init = 3):\n",
    "        \"\"\" \n",
    "        Initialize parameters of the LSTM (both weights and biases in one matrix) \n",
    "        One might way to have a positive fancy_forget_bias_init number (e.g. maybe even up to 5, in some papers)\n",
    "        \"\"\"\n",
    "        # +1 for the biases, which will be the first row of WLSTM\n",
    "        WLSTM = np.random.randn(input_size + hidden_size + 1, 4 * hidden_size) / np.sqrt(input_size + hidden_size)\n",
    "        WLSTM[0,:] = 0 # initialize biases to zero\n",
    "        if fancy_forget_bias_init != 0:\n",
    "            # forget gates get little bit negative bias initially to encourage them to be turned off\n",
    "            # remember that due to Xavier initialization above, the raw output activations from gates before\n",
    "            # nonlinearity are zero mean and on order of standard deviation ~1\n",
    "            WLSTM[0,hidden_size:2*hidden_size] = fancy_forget_bias_init\n",
    "        return WLSTM\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(X, WLSTM, c0 = None, h0 = None):\n",
    "        \"\"\"\n",
    "        X should be of shape (n,b,input_size), where n = length of sequence, b = batch size\n",
    "        \"\"\"\n",
    "        n,b,input_size = X.shape\n",
    "        d = WLSTM.shape[1]/4 # hidden size\n",
    "        if c0 is None: c0 = np.zeros((b,d))\n",
    "        if h0 is None: h0 = np.zeros((b,d))\n",
    "\n",
    "        # Perform the LSTM forward pass with X as the input\n",
    "        xphpb = WLSTM.shape[0] # x plus h plus bias, lol\n",
    "        Hin = np.zeros((n, b, xphpb)) # input [1, xt, ht-1] to each tick of the LSTM\n",
    "        Hout = np.zeros((n, b, d)) # hidden representation of the LSTM (gated cell content)\n",
    "        IFOG = np.zeros((n, b, d * 4)) # input, forget, output, gate (IFOG)\n",
    "        IFOGf = np.zeros((n, b, d * 4)) # after nonlinearity\n",
    "        C = np.zeros((n, b, d)) # cell content\n",
    "        Ct = np.zeros((n, b, d)) # tanh of cell content\n",
    "        for t in range(n):\n",
    "            # concat [x,h] as input to the LSTM\n",
    "            prevh = Hout[t-1] if t > 0 else h0\n",
    "            Hin[t,:,0] = 1 # bias\n",
    "            Hin[t,:,1:input_size+1] = X[t]\n",
    "            Hin[t,:,input_size+1:] = prevh\n",
    "            # compute all gate activations. dots: (most work is this line)\n",
    "            IFOG[t] = Hin[t].dot(WLSTM)\n",
    "            # non-linearities\n",
    "            IFOGf[t,:,:3*d] = 1.0/(1.0+np.exp(-IFOG[t,:,:3*d])) # sigmoids; these are the gates\n",
    "            IFOGf[t,:,3*d:] = np.tanh(IFOG[t,:,3*d:]) # tanh\n",
    "            # compute the cell activation\n",
    "            prevc = C[t-1] if t > 0 else c0\n",
    "            C[t] = IFOGf[t,:,:d] * IFOGf[t,:,3*d:] + IFOGf[t,:,d:2*d] * prevc\n",
    "            Ct[t] = np.tanh(C[t])\n",
    "            Hout[t] = IFOGf[t,:,2*d:3*d] * Ct[t]\n",
    "\n",
    "        cache = {}\n",
    "        cache['WLSTM'] = WLSTM\n",
    "        cache['Hout'] = Hout\n",
    "        cache['IFOGf'] = IFOGf\n",
    "        cache['IFOG'] = IFOG\n",
    "        cache['C'] = C\n",
    "        cache['Ct'] = Ct\n",
    "        cache['Hin'] = Hin\n",
    "        cache['c0'] = c0\n",
    "        cache['h0'] = h0\n",
    "\n",
    "        # return C[t], as well so we can continue LSTM with prev state init if needed\n",
    "        return Hout, C[t], Hout[t], cache\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(dHout_in, cache, dcn = None, dhn = None): \n",
    "\n",
    "        WLSTM = cache['WLSTM']\n",
    "        Hout = cache['Hout']\n",
    "        IFOGf = cache['IFOGf']\n",
    "        IFOG = cache['IFOG']\n",
    "        C = cache['C']\n",
    "        Ct = cache['Ct']\n",
    "        Hin = cache['Hin']\n",
    "        c0 = cache['c0']\n",
    "        h0 = cache['h0']\n",
    "        n,b,d = Hout.shape\n",
    "        input_size = WLSTM.shape[0] - d - 1 # -1 due to bias\n",
    "\n",
    "        # backprop the LSTM\n",
    "        dIFOG = np.zeros(IFOG.shape)\n",
    "        dIFOGf = np.zeros(IFOGf.shape)\n",
    "        dWLSTM = np.zeros(WLSTM.shape)\n",
    "        dHin = np.zeros(Hin.shape)\n",
    "        dC = np.zeros(C.shape)\n",
    "        dX = np.zeros((n,b,input_size))\n",
    "        dh0 = np.zeros((b, d))\n",
    "        dc0 = np.zeros((b, d))\n",
    "        dHout = dHout_in.copy() # make a copy so we don't have any funny side effects\n",
    "        if dcn is not None: dC[n-1] += dcn.copy() # carry over gradients from later\n",
    "        if dhn is not None: dHout[n-1] += dhn.copy()\n",
    "            \n",
    "        for t in reversed(range(n)):\n",
    "\n",
    "            tanhCt = Ct[t]\n",
    "            dIFOGf[t,:,2*d:3*d] = tanhCt * dHout[t]\n",
    "            # backprop tanh non-linearity first then continue backprop\n",
    "            dC[t] += (1-tanhCt**2) * (IFOGf[t,:,2*d:3*d] * dHout[t])\n",
    "\n",
    "            if t > 0:\n",
    "                dIFOGf[t,:,d:2*d] = C[t-1] * dC[t]\n",
    "                dC[t-1] += IFOGf[t,:,d:2*d] * dC[t]\n",
    "            else:\n",
    "                dIFOGf[t,:,d:2*d] = c0 * dC[t]\n",
    "                dc0 = IFOGf[t,:,d:2*d] * dC[t]\n",
    "                \n",
    "            dIFOGf[t,:,:d] = IFOGf[t,:,3*d:] * dC[t]\n",
    "            dIFOGf[t,:,3*d:] = IFOGf[t,:,:d] * dC[t]\n",
    "\n",
    "            # backprop activation functions\n",
    "            dIFOG[t,:,3*d:] = (1 - IFOGf[t,:,3*d:] ** 2) * dIFOGf[t,:,3*d:]\n",
    "            y = IFOGf[t,:,:3*d]\n",
    "            dIFOG[t,:,:3*d] = (y*(1.0-y)) * dIFOGf[t,:,:3*d]\n",
    "\n",
    "            # backprop matrix multiply\n",
    "            dWLSTM += np.dot(Hin[t].transpose(), dIFOG[t])\n",
    "            dHin[t] = dIFOG[t].dot(WLSTM.transpose())\n",
    "\n",
    "            # backprop the identity transforms into Hin\n",
    "            dX[t] = dHin[t,:,1:input_size+1]\n",
    "            if t > 0:\n",
    "                dHout[t-1,:] += dHin[t,:,input_size+1:]\n",
    "            else:\n",
    "                dh0 += dHin[t,:,input_size+1:]\n",
    "\n",
    "        return dX, dWLSTM, dc0, dh0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# TEST CASES\n",
    "# -------------------\n",
    "\n",
    "def checkSequentialMatchesBatch():\n",
    "    \"\"\" check LSTM I/O forward/backward interactions \"\"\"\n",
    "\n",
    "    n,b,d = (5, 3, 4) # sequence length, batch size, hidden size\n",
    "    input_size = 10\n",
    "    WLSTM = LSTM.init(input_size, d) # input size, hidden size\n",
    "    X = np.random.randn(n,b,input_size)\n",
    "    h0 = np.random.randn(b,d)\n",
    "    c0 = np.random.randn(b,d)\n",
    "\n",
    "    # sequential forward\n",
    "    cprev = c0\n",
    "    hprev = h0\n",
    "    caches = [{} for t in range(n)]\n",
    "    Hcat = np.zeros((n,b,d))\n",
    "    for t in range(n):\n",
    "        xt = X[t:t+1]\n",
    "        _, cprev, hprev, cache = LSTM.forward(xt, WLSTM, cprev, hprev)\n",
    "        caches[t] = cache\n",
    "        Hcat[t] = hprev\n",
    "\n",
    "    # sanity check: perform batch forward to check that we get the same thing\n",
    "    H, _, _, batch_cache = LSTM.forward(X, WLSTM, c0, h0)\n",
    "    assert np.allclose(H, Hcat), 'Sequential and Batch forward don''t match!'\n",
    "\n",
    "    # eval loss\n",
    "    wrand = np.random.randn(*Hcat.shape)\n",
    "    loss = np.sum(Hcat * wrand)\n",
    "    dH = wrand\n",
    "\n",
    "    # get the batched version gradients\n",
    "    BdX, BdWLSTM, Bdc0, Bdh0 = LSTM.backward(dH, batch_cache)\n",
    "\n",
    "    # now perform sequential backward\n",
    "    dX = np.zeros_like(X)\n",
    "    dWLSTM = np.zeros_like(WLSTM)\n",
    "    dc0 = np.zeros_like(c0)\n",
    "    dh0 = np.zeros_like(h0)\n",
    "    dcnext = None\n",
    "    dhnext = None\n",
    "    for t in reversed(range(n)):\n",
    "        dht = dH[t].reshape(1, b, d)\n",
    "        dx, dWLSTMt, dcprev, dhprev = LSTM.backward(dht, caches[t], dcnext, dhnext)\n",
    "        dhnext = dhprev\n",
    "        dcnext = dcprev\n",
    "\n",
    "        dWLSTM += dWLSTMt # accumulate LSTM gradient\n",
    "        dX[t] = dx[0]\n",
    "        if t == 0:\n",
    "            dc0 = dcprev\n",
    "            dh0 = dhprev\n",
    "\n",
    "    # and make sure the gradients match\n",
    "    print ('Making sure batched version agrees with sequential version: (should all be True)')\n",
    "    print (np.allclose(BdX, dX))\n",
    "    print (np.allclose(BdWLSTM, dWLSTM))\n",
    "    print (np.allclose(Bdc0, dc0))\n",
    "    print (np.allclose(Bdh0, dh0))\n",
    "    \n",
    "def checkBatchGradient():\n",
    "    \"\"\" check that the batch gradient is correct \"\"\"\n",
    "\n",
    "    # lets gradient check this beast\n",
    "    n,b,d = (5, 3, 4) # sequence length, batch size, hidden size\n",
    "    input_size = 10\n",
    "    WLSTM = LSTM.init(input_size, d) # input size, hidden size\n",
    "    X = np.random.randn(n,b,input_size)\n",
    "    h0 = np.random.randn(b,d)\n",
    "    c0 = np.random.randn(b,d)\n",
    "\n",
    "    # batch forward backward\n",
    "    H, Ct, Ht, cache = LSTM.forward(X, WLSTM, c0, h0)\n",
    "    wrand = np.random.randn(*H.shape)\n",
    "    loss = np.sum(H * wrand) # weighted sum is a nice hash to use I think\n",
    "    dH = wrand\n",
    "    dX, dWLSTM, dc0, dh0 = LSTM.backward(dH, cache)\n",
    "\n",
    "    def fwd():\n",
    "        h,_,_,_ = LSTM.forward(X, WLSTM, c0, h0)\n",
    "        return np.sum(h * wrand)\n",
    "\n",
    "    # now gradient check all\n",
    "    delta = 1e-5\n",
    "    rel_error_thr_warning = 1e-2\n",
    "    rel_error_thr_error = 1\n",
    "    tocheck = [X, WLSTM, c0, h0]\n",
    "    grads_analytic = [dX, dWLSTM, dc0, dh0]\n",
    "    names = ['X', 'WLSTM', 'c0', 'h0']\n",
    "    for j in range(len(tocheck)):\n",
    "        mat = tocheck[j]\n",
    "        dmat = grads_analytic[j]\n",
    "        name = names[j]\n",
    "        # gradcheck\n",
    "        for i in range(mat.size):\n",
    "            old_val = mat.flat[i]\n",
    "            mat.flat[i] = old_val + delta\n",
    "            loss0 = fwd()\n",
    "            mat.flat[i] = old_val - delta\n",
    "            loss1 = fwd()\n",
    "            mat.flat[i] = old_val\n",
    "\n",
    "            grad_analytic = dmat.flat[i]\n",
    "            grad_numerical = (loss0 - loss1) / (2 * delta)\n",
    "\n",
    "            if grad_numerical == 0 and grad_analytic == 0:\n",
    "                rel_error = 0 # both are zero, OK.\n",
    "                status = 'OK'\n",
    "            elif abs(grad_numerical) < 1e-7 and abs(grad_analytic) < 1e-7:\n",
    "                rel_error = 0 # not enough precision to check this\n",
    "                status = 'VAL SMALL WARNING'\n",
    "            else:\n",
    "                rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "                status = 'OK'\n",
    "                if rel_error > rel_error_thr_warning: status = 'WARNING'\n",
    "                if rel_error > rel_error_thr_error: status = '!!!!! NOTOK'\n",
    "\n",
    "        # print stats\n",
    "        print ('%s checking param %s index %s (val = %+8f), analytic = %+8f, numerical = %+8f, relative error = %+8f' \n",
    "            % (status, name, np.unravel_index(i, mat.shape), old_val, grad_analytic, grad_numerical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cc872bc0724e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheckSequentialMatchesBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'check OK, press key to continue to gradient check'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcheckBatchGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'every line should start with OK. Have a nice day!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-bad99ae97650>\u001b[0m in \u001b[0;36mcheckSequentialMatchesBatch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mHcat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-bb554e3f0fa3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(X, WLSTM, c0, h0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mxphpb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# x plus h plus bias, lol\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mHin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxphpb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input [1, xt, ht-1] to each tick of the LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mHout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hidden representation of the LSTM (gated cell content)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mIFOG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input, forget, output, gate (IFOG)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mIFOGf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# after nonlinearity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "checkSequentialMatchesBatch()\n",
    "raw_input('check OK, press key to continue to gradient check')\n",
    "checkBatchGradient()\n",
    "print ('every line should start with OK. Have a nice day!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, agrega en esta misma celda comentarios sobre la comparación entre los dos modelos vistos, diferencias, similitudes, ventajas, desventajas. Recuerda es una opinión personal basada en tu trabajo, no pongas lo que dice la literatura si no lo que tu experimentaste a la hora de desarrollar y aplicar los modelos, así no concuerde con lo que leas en otros lados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
